\chapter{Background}
This chapter provides the necessary background on stencil computations and the unique architectural features of the Cerebras Wafer-Scale Engine (\ac{wse}).
\subsection{Stencil Computations}
Stencil computations are a class of algorithms defined by a regular data access pattern on a grid.
The value of every cell is iteratively updated based on a fixed pattern of its neighbors' values from the previous step.
This pattern is known as the stencil.

They can be categorized by several properties, including the dimensionality of the grid, the shape and locality of the stencil, and the boundary conditions applied.
This thesis focuses on a specific, yet widely applicable, category: \textbf{linear, two-dimensional, star-shaped stencils}.
This means the computation is performed on a 2D grid, where each cell is updated using only its axis-aligned neighbors (i.e., no diagonals), and the the new cell value is a linear combination of its previous value and its neighbors' values.
Stencils that also include diagonal neighbors are referred to as box-shaped stencils.

The boundary condition defines how cells at the edges of the grid are handled.
A widely used example is the Dirichlet boundary condition, where edge cells are held to a constant value.
This is the sole boundary condition used in this work.

Stencils are further defined by their radius, which specifies the maximum distance between the center cell and any neighbor used in the update.
We can further categorize the stencils by the parameters.
The stencil's coefficients (or weights) can also be characterized:

\begin{itemize}
    \item If the coefficients for all neighbor cells depend only on their distance from the center, the stencil is symmetric. Otherwise, it is asymmetric. Asymmetric stencils are crucial for modeling direction-dependent phenomena, such as in advection problems.
    \item If the coefficients are the same for all cells across the grid, they are constant. If they depend on the cell's absolute position, they are variable coefficients, used to model problems with spatially varying properties, like heterogeneous materials.
\end{itemize}

A primary application of 2D star-shaped stencils is the finite-difference discretization of the Laplacian operator, which is fundamental to solving \acp{pde} like the Heat Equation and Poisson's Equation. The Heat Equation describes temperature diffusion, with applications in semiconductor chip design, while Poisson's Equation is used in fields like electrostatics and fluid dynamics.

Stencil codes can be broadly classified by their iterative method. In time-stepping applications, each full-grid update (iteration) represents a discrete step forward in time. Such applications are typically run for a fixed number of iterations. The simplest method for this, which updates the entire grid based on the previous state, is the Jacobi method. Its perfectly parallel nature makes it an ideal candidate for massively parallel architectures.

In contrast, relaxation methods iterate until the grid converges to a steady state. Since the notion of discrete time steps is less strict, methods like the Gauss-Seidel or multigrid methods can be employed to accelerate convergence. These methods often introduce dependencies that break the perfect parallelism of the Jacobi method. While the \ac{wse} could be adapted for such methods, they typically require a global convergence check (an all-reduce operation), which is beyond the scope of this thesis. Therefore, this work focuses exclusively on Jacobi-style updates.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{stencil_visualization.png}
    \caption{A 2D star-shaped stencil pattern with radius 2}
    \label{fig:stencil_visualization}
\end{figure}

\subsection{The Cerebras Wafer-Scale Engine (\ac{wse})}
The Cerebras \ac{wse} has very distinct characteristics compared to both \acp{cpu} and \acp{gpu}.
Instead of seperated compute cores and memory, Cerebras \ac{wse} features several hundred thousand \ac{ce} that each consist of \qty{48}{\kilo\byte} of memory and a compute core. In contrast to traditional hardware, the memory consist soley of ultra fast \ac{sram} with a bandwidth that allows to read \qty{16}{\byte} and write \qty{8}{\byte} per cycle. It is layed out into eight banks with specific restrictions on which banks can be accessed at the same time.


For communication each \ac{pe} also contains a fabric router additional to the \ac{ce}. The fabric router has a bidirection link to the routers of the four neighbouring \acp{pe} as well as to the \ac{ce}. Each of these links has a bidirectional bandwidth of \qty{32}{\bit} per cycle. For the \ac{wse}-2 \numproduct{66 x 154} \acp{pe} form a die and \numproduct{12 x 7} die reside on one \qty{300}{\mm} wafer forming the \ac{wse}-2 with staggering \num{853104} physical \acp{pe}. Distinct from traditional production processes the die are not cut, but kept together on the waver and are linked so that the communication between \ac{pe} no only works within each die but also in between dies.
The data flow between \acp{pe} can be defined through 24 static routes called colors.
Due to not perfect yiel, some \acp{pe} are non functional.
Cerebras solves this problem by routing around these \acp{pe}. 
To make this work, about \num{100000} \acp{pe} are spare \acp{pe} so that the logical number of usable \acp{pe} is a lot lower.
As suggested by the documentation \cite{cerebras_gemv_tutorial}, usable fabric dimensions might slightly change from system to system. Tramm et al. \cite{tramm2024efficient} find the user programmable number of \acp{pe} for \ac{wse}-2 to be \numproduct{750 x 994}, so we use this number throughout our work. For \ac{wse}-3 the official documentation states the fabric dimensions to be \numproduct{762 x 1176} and while this does not include the spare cores, it is not clear what the exact user programmable number is. In lack of official numbers, we use these throughout our work. Both systems operate at a clock speed of \qty{1.1}{\giga\hertz}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{wse2_pes_router.png}
    \caption{The \ac{wse}-2 has a grid like arrangement of \acp{pe} with a router for each \ac{pe}. The router has a link to the routers of the four neighbouring \acp{pe} as well as to the \ac{ce}. Each of these links has a bidirectional bandwidth of \qty{32}{\bit} per cycle \cite{lie2023cerebras}}
    \label{fig:wse2_pes_router}
\end{figure}

Each \ac{pe} allows to execute certain instructions in parallel with \acp{simd} units. The maximum number of parallel instructions varies between instruction types and \ac{wse} generations. Table \ref{tab:simd_operations} shows the maximum \acp{simd}-width for selected instruction types. As the \ac{wse} was originally designed for \ac{ai} workloads, the \acp{simd} units are optimized for half precision floating point operations, which is are not suitable for \ac{hpc} workloads.

\begin{table}[h]
    \centering
    \caption{Maximum \acp{simd}-width for selected instruction types}
    \label{tab:simd_operations}
    \begin{tabular}{@{}cccc@{}}
        \toprule
        Op code & desciption & \ac{wse}-2 & \ac{wse}-3 \\
        \midrule
        \texttt{@fadds} & 32-bit floating point add & 2 & 4 \\
        \texttt{@fmuls} & 32-bit floating point multiply & 1 & 1 \\
        \texttt{@fmach} & 16-bit floating point multiply-add & 4 & 8 \\
        \texttt{@fmachs} & 16-bit floating point multiply with 32 bit addition & 2 & 4 \\
        \texttt{@fmacs} & 32-bit floating point multiply-add & 1 & 1 \\
        \texttt{@fmovs} & 32-bit floating point move & 2 & 4 \\
        \bottomrule
    \end{tabular}
\end{table}

Custom kernels for the \ac{wse} are written in \ac{csl} which is very low level language based on Zig and extends the language with hardware specific features.
One of its core features is the ability to define \acp{dsd} which define data access patterns including a base memory address, an offset, a stride and a length. They can be used for up to four dimensional tensors. Specifically there are four \ac{dsd} types. \texttt{mem1d\_dsd}, \texttt{mem4d\_dsd}, \texttt{fabin\_dsd} and \texttt{fabout\_dsd}. While \texttt{fabin\_dsd} and \texttt{fabout\_dsd} are used for fabric communication and \texttt{mem1d\_dsd} are used for one dimensional arrays, \texttt{mem4d\_dsd} are somewhat unintuitively used for tensors with two, three or four dimensions. The \ac{wse}-2 has 44 \acp{dsr} per \ac{pe} which are hardware registers used to hold \acp{dsd}. These can be directly used by as operands for instructions. Loading \acp{dsd} into \acp{dsr} is done automatically by the compiler, but can also be done manually and enables the programmer to optimize the code. Communication between \acp{pe} is handled with special fabin- and fabout-\acp{dsd}, describing data that is sent to or received from a neighbouring \ac{pe}.

The routers contain a limited number of input and output queues, small physical buffers for incoming and outgoing data.

\ac{csl} has limited support for concurrency which is enabled by the use of tasks that are activated by events like completion of an asynchronous communication or computation. Due to the single thread of execution, there is no true parallelism within a single \ac{pe}.

optional: ???
- cannot receive and send at same time???
- limitations due to input and output queues? 
